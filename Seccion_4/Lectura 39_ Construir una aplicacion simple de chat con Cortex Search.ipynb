{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "27pstaar6oslyuo62ivm",
   "authorId": "5639445461508",
   "authorName": "JOSE",
   "authorEmail": "jmoyacurbelo@outlook.com",
   "sessionId": "c9c7d361-4bc8-428b-9e38-9018d76837ac",
   "lastEditTime": 1749515755041
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e79277b-8743-405f-9981-b5051fe8e842",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "En esta lecci贸n veremos c贸mo utilizar Cortex Search y la funci贸n `COMPLETE (SNOWFLAKE.CORTEX)` para configurar un chatbot de recuperaci贸n-generaci贸n aumentada (RAG) en Snowflake.\n\n### Datos\n\nPara esta lecci贸n, utilizaremos un conjunto de datos que se encuentra en Kaggle. El conjunto de datos \"Libros\" contiene el nombre, el t铆tulo y la descripci贸n de los libros. Puede descargarlo desde el siguiente enlace: [Descargar datos](https://www.kaggle.com/datasets/elvinrustam/books-dataset/data)\n\n### Crear el warehouse y el stage"
  },
  {
   "cell_type": "code",
   "id": "6d8cabf8-171a-4d30-846b-9d6a75242cc8",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- Crear el warehouse\nCREATE OR REPLACE WAREHOUSE cortex_search_app_wh WITH\n    WAREHOUSE_SIZE='X-SMALL'\n    AUTO_SUSPEND = 120\n    AUTO_RESUME = TRUE\n    INITIALLY_SUSPENDED=TRUE;\n\nUSE WAREHOUSE cortex_search_app_wh;\n\n-- Crear el stage\nCREATE OR REPLACE STAGE curso_ia.seccion_4.datos_libros_stage\n    DIRECTORY = (ENABLE = TRUE)\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43bca71b-d3b6-4da7-92f6-2ef21ca27f0f",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Ingestar los datos a Snowflake\n\nCreamos una tabla de nombre `libros` para guardar la informaci贸n."
  },
  {
   "cell_type": "code",
   "id": "261c2b25-0a62-4b99-bb4d-f708aa83749d",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE curso_ia.seccion_4.libros (\n    title VARCHAR(500),\n    authors VARCHAR(1000),\n    description TEXT,\n    category VARCHAR(200),\n    publisher VARCHAR(300),\n    price_starting_with DECIMAL(10,2),\n    publish_date_month VARCHAR(20),\n    publish_date_year INTEGER\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c97877-eba4-4bd7-983f-0a69cfe1c57e",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "Ingestamos la informaci贸n en la tabla."
  },
  {
   "cell_type": "code",
   "id": "8b8ccccb-585f-4b5a-b535-5ba078b1a0c9",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Cargar datos en la tabla libros\nCOPY INTO curso_ia.seccion_4.libros\nFROM @curso_ia.seccion_4.datos_libros_stage/dataset.csv\nFILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1 FIELD_OPTIONALLY_ENCLOSED_BY ='\"');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60021a34-4fb7-4faf-8172-cbbae77db915",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Consultamos la tabla."
  },
  {
   "cell_type": "code",
   "id": "7a3d3583-0ac0-4c7d-8408-b41c118d0240",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "SELECT * FROM curso_ia.seccion_4.libros;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "751f3351-2384-472e-a316-67c763d25882",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### Crear una UDF de fragmentaci贸n de texto\n\nComo hemos comentado proporcionarle documentos largos a Cortex Search no es eficiente, ya que los modelos de recuperaci贸n funcionan mejor con fragmentos peque帽os de texto. A continuaci贸n, crearemos un UDF de Python para fragmentar el texto."
  },
  {
   "cell_type": "code",
   "id": "5c907bd9-f6c8-4d74-8205-8ae8ea30ce30",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION curso_ia.seccion_4.books_chunk(\n    description string, title string, authors string, category string, publisher string\n)\n    returns table (chunk string, title string, authors string, category string, publisher string)\n    language python\n    runtime_version = '3.9'\n    handler = 'text_chunker'\n    packages = ('snowflake-snowpark-python','langchain')\n    as\n$$\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport copy\nfrom typing import Optional\n\nclass text_chunker:\n\n    def process(self, description: Optional[str], title: str, authors: str, category: str, publisher: str):\n        if description == None:\n            description = \"\" # handle null values\n\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 2000,\n            chunk_overlap  = 300,\n            length_function = len\n        )\n        chunks = text_splitter.split_text(description)\n        for chunk in chunks:\n            yield (title + \"\\n\" + authors + \"\\n\" + chunk, title, authors, category, publisher) # always chunk with title\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49cc62a6-c399-4e78-b4f2-80d16075e993",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Crear la tabla de fragmentos de texto\n\nA continuaci贸n vamos a crear una tabla para almacenar los fragmentos de texto extra铆dos de las descripciones de los libros."
  },
  {
   "cell_type": "code",
   "id": "eaf7d5f5-2c52-46c7-b5fb-8543c985e795",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "CREATE TABLE curso_ia.seccion_4.descripcion_libros_chunks AS (\n    SELECT\n        libros.*,\n        t.CHUNK as CHUNK\n    FROM curso_ia.seccion_4.libros libros,\n        TABLE(curso_ia.seccion_4.books_chunk(libros.description, libros.title, libros.authors, libros.category, libros.publisher)) t\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69751bab-72b0-4f75-bf42-8bd2d7b20362",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "Verificamos el contenido de la tabla creada."
  },
  {
   "cell_type": "code",
   "id": "77e13a34-0e3a-4038-b981-9f264b21ea1f",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "SELECT chunk, * FROM curso_ia.seccion_4.descripcion_libros_chunks LIMIT 50;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fefb3a8c-bc5e-4991-958d-a9470636c886",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "### Crear un servicio de Cortex Search\n\nVamos a crear un servicio de b煤squeda de Cortex en la tabla que acabamos de crear para poder buscar en los fragmentos de `descripcion_libros_chunks.`"
  },
  {
   "cell_type": "code",
   "id": "d572772d-8561-4cc7-bfeb-9277ab5be263",
   "metadata": {
    "language": "sql",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE curso_ia.seccion_4.libros_service\n    ON CHUNK\n    WAREHOUSE = search_service_wh\n    TARGET_LAG = '1 day',\n    EMBEDDING_MODEL = 'voyage-multilingual-2'\n    AS (\n        SELECT *\n        FROM curso_ia.seccion_4.descripcion_libros_chunks\n    );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ae7347d-5089-4797-9bb1-216ab8b251f5",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "### Crear una aplicaci贸n de Streamlit\n\nA continuaci贸n deber谩 ir a la secci贸n de Streamlit y seguir los siguientes pasos para crear la aplicaci贸n:\n\n1. Click en + Streamlit app\n2. Proporcionarle el nombre `book_app`\n3. Seleccionar la base de datos, esquema y warehouse:\n    \n    Base de datos: `curso_ia`\n    \n    Esquema: `seccion_4`\n    \n4. Click en Create\n5. Eliminar el c贸digo de ejemplo que trae la plantilla\n6. Copiar el c贸digo Python de la siguiente celda en la aplicaci贸n de Streamlit.\n7. Agregar las librer铆as necesarias. Para esta aplicaci贸n debemos asegurarnos de que tenemos agregada las siguientes librer铆as:\n    - `snowflake`\n    - `snowflake-snowpark-python`\n8. Ejecutar la aplicaci贸n"
  },
  {
   "cell_type": "code",
   "id": "e2dca69c-78ca-4cbc-9c3d-e214c436276d",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\n\nMODELS = [\n    \"claude-4-sonnet\",\n    \"openai-gpt-4.1\",\n    \"mistral-large\",\n    \"snowflake-arctic\",\n    \"llama3-70b\",\n    \"llama3-8b\",\n]\n\ndef init_messages():\n    \"\"\"\n    Inicializa el estado de sesi贸n para los mensajes del chat. Si el estado de sesi贸n indica que la\n    conversaci贸n debe ser borrada o si la clave \"messages\" no est谩 en el estado de sesi贸n,\n    la inicializa como una lista vac铆a.\n    \"\"\"\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\ndef init_service_metadata():\n    \"\"\"\n    Inicializa el estado de sesi贸n para los metadatos del servicio de b煤squeda cortex. Consulta los\n    servicios de b煤squeda cortex disponibles desde la sesi贸n de Snowflake y almacena sus nombres y\n    columnas de b煤squeda en el estado de sesi贸n.\n    \"\"\"\n    if \"service_metadata\" not in st.session_state:\n        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n        service_metadata = []\n        if services:\n            for s in services:\n                svc_name = s[\"name\"]\n                svc_search_col = session.sql(\n                    f\"DESC CORTEX SEARCH SERVICE {svc_name};\"\n                ).collect()[0][\"search_column\"]\n                service_metadata.append(\n                    {\"name\": svc_name, \"search_column\": svc_search_col}\n                )\n\n        st.session_state.service_metadata = service_metadata\n\ndef init_config_options():\n    \"\"\"\n    Inicializa las opciones de configuraci贸n en la barra lateral de Streamlit. Permite al usuario seleccionar\n    un servicio de b煤squeda cortex, borrar la conversaci贸n, alternar el modo debug y alternar el uso del\n    historial de chat. Tambi茅n proporciona opciones avanzadas para seleccionar un modelo, el n煤mero de\n    fragmentos de contexto y el n煤mero de mensajes de chat a usar en el historial de chat.\n    \"\"\"\n    st.sidebar.selectbox(\n        \"Seleccionar servicio de b煤squeda cortex:\",\n        [s[\"name\"] for s in st.session_state.service_metadata],\n        key=\"selected_cortex_search_service\",\n    )\n\n    st.sidebar.button(\"Borrar conversaci贸n\", key=\"clear_conversation\")\n    st.sidebar.toggle(\"Debug\", key=\"debug\", value=False)\n    st.sidebar.toggle(\"Usar historial de chat\", key=\"use_chat_history\", value=True)\n\n    with st.sidebar.expander(\"Opciones avanzadas\"):\n        st.selectbox(\"Seleccionar modelo:\", MODELS, key=\"model_name\")\n        st.number_input(\n            \"Seleccionar n煤mero de fragmentos de contexto\",\n            value=5,\n            key=\"num_retrieved_chunks\",\n            min_value=1,\n            max_value=10,\n        )\n        st.number_input(\n            \"Seleccionar n煤mero de mensajes a usar en el historial de chat\",\n            value=5,\n            key=\"num_chat_messages\",\n            min_value=1,\n            max_value=10,\n        )\n\n    st.sidebar.expander(\"Estado de Sesi贸n\").write(st.session_state)\n\ndef query_cortex_search_service(query):\n    \"\"\"\n    Consulta el servicio de b煤squeda cortex seleccionado con la consulta dada y recupera documentos de contexto.\n    Muestra los documentos de contexto recuperados en la barra lateral si el modo debug est谩 habilitado. Devuelve\n    los documentos de contexto como una cadena.\n\n    Args:\n        query (str): La consulta para buscar en el servicio de b煤squeda cortex.\n\n    Returns:\n        str: La cadena concatenada de documentos de contexto.\n    \"\"\"\n    db, schema = session.get_current_database(), session.get_current_schema()\n\n    cortex_search_service = (\n        root.databases[db]\n        .schemas[schema]\n        .cortex_search_services[st.session_state.selected_cortex_search_service]\n    )\n\n    context_documents = cortex_search_service.search(\n        query, columns=[], limit=st.session_state.num_retrieved_chunks\n    )\n    results = context_documents.results\n\n    service_metadata = st.session_state.service_metadata\n    search_col = [s[\"search_column\"] for s in service_metadata\n                    if s[\"name\"] == st.session_state.selected_cortex_search_service][0]\n\n    context_str = \"\"\n    for i, r in enumerate(results):\n        context_str += f\"Documento de contexto {i+1}: {r[search_col]} \\n\" + \"\\n\"\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\"Documentos de contexto\", context_str, height=500)\n\n    return context_str\n\ndef get_chat_history():\n    \"\"\"\n    Recupera el historial de chat del estado de sesi贸n limitado al n煤mero de mensajes especificado\n    por el usuario en las opciones de la barra lateral.\n\n    Returns:\n        list: La lista de mensajes de chat del estado de sesi贸n.\n    \"\"\"\n    start_index = max(\n        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n    )\n    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n\ndef complete(model, prompt):\n    \"\"\"\n    Genera una respuesta para el prompt dado usando el modelo especificado.\n\n    Args:\n        model (str): El nombre del modelo a usar para la respuesta.\n        prompt (str): El prompt para generar una respuesta.\n\n    Returns:\n        str: La respuesta generada.\n    \"\"\"\n    return session.sql(\"SELECT ai_complete(?,?)\", (model, prompt)).collect()[0][0]\n\ndef make_chat_history_summary(chat_history, question):\n    \"\"\"\n    Genera un resumen del historial de chat combinado con la pregunta actual para extender el contexto\n    de la consulta. Usa el modelo de lenguaje para generar este resumen.\n\n    Args:\n        chat_history (str): El historial de chat a incluir en el resumen.\n        question (str): La pregunta actual del usuario para extender con el historial de chat.\n\n    Returns:\n        str: El resumen generado del historial de chat y la pregunta.\n    \"\"\"\n    prompt = f\"\"\"\n        [INST]\n        Bas谩ndote en el historial de chat a continuaci贸n y la pregunta, genera una consulta que extienda la pregunta\n        con el historial de chat proporcionado. La consulta debe estar en lenguaje natural.\n        Responde solo con la consulta. No agregues ninguna explicaci贸n.\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        [/INST]\n    \"\"\"\n\n    summary = complete(st.session_state.model_name, prompt)\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\n            \"Resumen del historial de chat\", summary.replace(\"$\", \"\\$\"), height=150\n        )\n\n    return summary\n\ndef create_prompt(user_question):\n    \"\"\"\n    Crea un prompt para el modelo de lenguaje combinando la pregunta del usuario con el contexto recuperado\n    del servicio de b煤squeda cortex y el historial de chat (si est谩 habilitado). Formatea el prompt de acuerdo\n    al formato de entrada esperado del modelo.\n\n    Args:\n        user_question (str): La pregunta del usuario para generar un prompt.\n\n    Returns:\n        str: El prompt generado para el modelo de lenguaje.\n    \"\"\"\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n        if chat_history != []:\n            question_summary = make_chat_history_summary(chat_history, user_question)\n            prompt_context = query_cortex_search_service(question_summary)\n        else:\n            prompt_context = query_cortex_search_service(user_question)\n    else:\n        prompt_context = query_cortex_search_service(user_question)\n        chat_history = \"\"\n\n    prompt = f\"\"\"\n            [INST]\n            Eres un asistente de chat de IA 煤til con capacidades RAG. Cuando un usuario te haga una pregunta,\n            tambi茅n se te dar谩 contexto proporcionado entre las etiquetas <context> y </context>. Usa ese contexto\n            con el historial de chat del usuario proporcionado entre las etiquetas <chat_history> y </chat_history>\n            para proporcionar un resumen que aborde la pregunta del usuario. Aseg煤rate de que la respuesta sea coherente, concisa\n            y directamente relevante a la pregunta del usuario.\n\n            Si el usuario hace una pregunta gen茅rica que no puede ser respondida con el contexto dado o el chat_history,\n            simplemente di \"No conozco la respuesta a esa pregunta.\"\n\n            No digas cosas como \"seg煤n el contexto proporcionado\".\n\n            <chat_history>\n            {chat_history}\n            </chat_history>\n            <context>\n            {prompt_context}\n            </context>\n            <question>\n            {user_question}\n            </question>\n            [/INST]\n            Respuesta:\n        \"\"\"\n    return prompt\n\ndef main():\n    st.title(f\":speech_balloon: Chatbot con Snowflake Cortex\")\n\n    init_service_metadata()\n    init_config_options()\n    init_messages()\n\n    icons = {\"assistant\": \"锔\", \"user\": \"\"}\n\n    # Mostrar mensajes de chat del historial al reejecutar la aplicaci贸n\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n            st.markdown(message[\"content\"])\n\n    disable_chat = (\n        \"service_metadata\" not in st.session_state\n        or len(st.session_state.service_metadata) == 0\n    )\n    if question := st.chat_input(\"Haz una pregunta...\", disabled=disable_chat):\n        # Agregar mensaje del usuario al historial de chat\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Mostrar mensaje del usuario en el contenedor de mensajes de chat\n        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n            st.markdown(question.replace(\"$\", \"\\$\"))\n\n        # Mostrar respuesta del asistente en el contenedor de mensajes de chat\n        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n            message_placeholder = st.empty()\n            question = question.replace(\"'\", \"\")\n            with st.spinner(\"Pensando...\"):\n                generated_response = complete(\n                    st.session_state.model_name, create_prompt(question)\n                )\n                message_placeholder.markdown(generated_response)\n\n        st.session_state.messages.append(\n            {\"role\": \"assistant\", \"content\": generated_response}\n        )\n\nif __name__ == \"__main__\":\n    session = get_active_session()\n    root = Root(session)\n    main()",
   "execution_count": null
  }
 ]
}