{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "27pstaar6oslyuo62ivm",
   "authorId": "5639445461508",
   "authorName": "JOSE",
   "authorEmail": "jmoyacurbelo@outlook.com",
   "sessionId": "c9c7d361-4bc8-428b-9e38-9018d76837ac",
   "lastEditTime": 1749515755041
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e79277b-8743-405f-9981-b5051fe8e842",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "En esta lección veremos cómo utilizar Cortex Search y la función `COMPLETE (SNOWFLAKE.CORTEX)` para configurar un chatbot de recuperación-generación aumentada (RAG) en Snowflake.\n\n### Datos\n\nPara esta lección, utilizaremos un conjunto de datos que se encuentra en Kaggle. El conjunto de datos \"Libros\" contiene el nombre, el título y la descripción de los libros. Puede descargarlo desde el siguiente enlace: [Descargar datos](https://www.kaggle.com/datasets/elvinrustam/books-dataset/data)\n\n### Crear el warehouse y el stage"
  },
  {
   "cell_type": "code",
   "id": "6d8cabf8-171a-4d30-846b-9d6a75242cc8",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- Crear el warehouse\nCREATE OR REPLACE WAREHOUSE cortex_search_app_wh WITH\n    WAREHOUSE_SIZE='X-SMALL'\n    AUTO_SUSPEND = 120\n    AUTO_RESUME = TRUE\n    INITIALLY_SUSPENDED=TRUE;\n\nUSE WAREHOUSE cortex_search_app_wh;\n\n-- Crear el stage\nCREATE OR REPLACE STAGE curso_ia.seccion_4.datos_libros_stage\n    DIRECTORY = (ENABLE = TRUE)\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43bca71b-d3b6-4da7-92f6-2ef21ca27f0f",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Ingestar los datos a Snowflake\n\nCreamos una tabla de nombre `libros` para guardar la información."
  },
  {
   "cell_type": "code",
   "id": "261c2b25-0a62-4b99-bb4d-f708aa83749d",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE curso_ia.seccion_4.libros (\n    title VARCHAR(500),\n    authors VARCHAR(1000),\n    description TEXT,\n    category VARCHAR(200),\n    publisher VARCHAR(300),\n    price_starting_with DECIMAL(10,2),\n    publish_date_month VARCHAR(20),\n    publish_date_year INTEGER\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c97877-eba4-4bd7-983f-0a69cfe1c57e",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "Ingestamos la información en la tabla."
  },
  {
   "cell_type": "code",
   "id": "8b8ccccb-585f-4b5a-b535-5ba078b1a0c9",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Cargar datos en la tabla libros\nCOPY INTO curso_ia.seccion_4.libros\nFROM @curso_ia.seccion_4.datos_libros_stage/dataset.csv\nFILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1 FIELD_OPTIONALLY_ENCLOSED_BY ='\"');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60021a34-4fb7-4faf-8172-cbbae77db915",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Consultamos la tabla."
  },
  {
   "cell_type": "code",
   "id": "7a3d3583-0ac0-4c7d-8408-b41c118d0240",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "SELECT * FROM curso_ia.seccion_4.libros;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "751f3351-2384-472e-a316-67c763d25882",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### Crear una UDF de fragmentación de texto\n\nComo hemos comentado proporcionarle documentos largos a Cortex Search no es eficiente, ya que los modelos de recuperación funcionan mejor con fragmentos pequeños de texto. A continuación, crearemos un UDF de Python para fragmentar el texto."
  },
  {
   "cell_type": "code",
   "id": "5c907bd9-f6c8-4d74-8205-8ae8ea30ce30",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION curso_ia.seccion_4.books_chunk(\n    description string, title string, authors string, category string, publisher string\n)\n    returns table (chunk string, title string, authors string, category string, publisher string)\n    language python\n    runtime_version = '3.9'\n    handler = 'text_chunker'\n    packages = ('snowflake-snowpark-python','langchain')\n    as\n$$\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport copy\nfrom typing import Optional\n\nclass text_chunker:\n\n    def process(self, description: Optional[str], title: str, authors: str, category: str, publisher: str):\n        if description == None:\n            description = \"\" # handle null values\n\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 2000,\n            chunk_overlap  = 300,\n            length_function = len\n        )\n        chunks = text_splitter.split_text(description)\n        for chunk in chunks:\n            yield (title + \"\\n\" + authors + \"\\n\" + chunk, title, authors, category, publisher) # always chunk with title\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49cc62a6-c399-4e78-b4f2-80d16075e993",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Crear la tabla de fragmentos de texto\n\nA continuación vamos a crear una tabla para almacenar los fragmentos de texto extraídos de las descripciones de los libros."
  },
  {
   "cell_type": "code",
   "id": "eaf7d5f5-2c52-46c7-b5fb-8543c985e795",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "CREATE TABLE curso_ia.seccion_4.descripcion_libros_chunks AS (\n    SELECT\n        libros.*,\n        t.CHUNK as CHUNK\n    FROM curso_ia.seccion_4.libros libros,\n        TABLE(curso_ia.seccion_4.books_chunk(libros.description, libros.title, libros.authors, libros.category, libros.publisher)) t\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69751bab-72b0-4f75-bf42-8bd2d7b20362",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "Verificamos el contenido de la tabla creada."
  },
  {
   "cell_type": "code",
   "id": "77e13a34-0e3a-4038-b981-9f264b21ea1f",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "SELECT chunk, * FROM curso_ia.seccion_4.descripcion_libros_chunks LIMIT 50;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fefb3a8c-bc5e-4991-958d-a9470636c886",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "### Crear un servicio de Cortex Search\n\nVamos a crear un servicio de búsqueda de Cortex en la tabla que acabamos de crear para poder buscar en los fragmentos de `descripcion_libros_chunks.`"
  },
  {
   "cell_type": "code",
   "id": "d572772d-8561-4cc7-bfeb-9277ab5be263",
   "metadata": {
    "language": "sql",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE curso_ia.seccion_4.libros_service\n    ON CHUNK\n    WAREHOUSE = search_service_wh\n    TARGET_LAG = '1 day',\n    EMBEDDING_MODEL = 'voyage-multilingual-2'\n    AS (\n        SELECT *\n        FROM curso_ia.seccion_4.descripcion_libros_chunks\n    );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ae7347d-5089-4797-9bb1-216ab8b251f5",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "### Crear una aplicación de Streamlit\n\nA continuación deberá ir a la sección de Streamlit y seguir los siguientes pasos para crear la aplicación:\n\n1. Click en + Streamlit app\n2. Proporcionarle el nombre `book_app`\n3. Seleccionar la base de datos, esquema y warehouse:\n    \n    Base de datos: `curso_ia`\n    \n    Esquema: `seccion_4`\n    \n4. Click en Create\n5. Eliminar el código de ejemplo que trae la plantilla\n6. Copiar el código Python de la siguiente celda en la aplicación de Streamlit.\n7. Agregar las librerías necesarias. Para esta aplicación debemos asegurarnos de que tenemos agregada las siguientes librerías:\n    - `snowflake`\n    - `snowflake-snowpark-python`\n8. Ejecutar la aplicación"
  },
  {
   "cell_type": "code",
   "id": "e2dca69c-78ca-4cbc-9c3d-e214c436276d",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\n\nMODELS = [\n    \"claude-4-sonnet\",\n    \"openai-gpt-4.1\",\n    \"mistral-large\",\n    \"snowflake-arctic\",\n    \"llama3-70b\",\n    \"llama3-8b\",\n]\n\ndef init_messages():\n    \"\"\"\n    Inicializa el estado de sesión para los mensajes del chat. Si el estado de sesión indica que la\n    conversación debe ser borrada o si la clave \"messages\" no está en el estado de sesión,\n    la inicializa como una lista vacía.\n    \"\"\"\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\ndef init_service_metadata():\n    \"\"\"\n    Inicializa el estado de sesión para los metadatos del servicio de búsqueda cortex. Consulta los\n    servicios de búsqueda cortex disponibles desde la sesión de Snowflake y almacena sus nombres y\n    columnas de búsqueda en el estado de sesión.\n    \"\"\"\n    if \"service_metadata\" not in st.session_state:\n        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n        service_metadata = []\n        if services:\n            for s in services:\n                svc_name = s[\"name\"]\n                svc_search_col = session.sql(\n                    f\"DESC CORTEX SEARCH SERVICE {svc_name};\"\n                ).collect()[0][\"search_column\"]\n                service_metadata.append(\n                    {\"name\": svc_name, \"search_column\": svc_search_col}\n                )\n\n        st.session_state.service_metadata = service_metadata\n\ndef init_config_options():\n    \"\"\"\n    Inicializa las opciones de configuración en la barra lateral de Streamlit. Permite al usuario seleccionar\n    un servicio de búsqueda cortex, borrar la conversación, alternar el modo debug y alternar el uso del\n    historial de chat. También proporciona opciones avanzadas para seleccionar un modelo, el número de\n    fragmentos de contexto y el número de mensajes de chat a usar en el historial de chat.\n    \"\"\"\n    st.sidebar.selectbox(\n        \"Seleccionar servicio de búsqueda cortex:\",\n        [s[\"name\"] for s in st.session_state.service_metadata],\n        key=\"selected_cortex_search_service\",\n    )\n\n    st.sidebar.button(\"Borrar conversación\", key=\"clear_conversation\")\n    st.sidebar.toggle(\"Debug\", key=\"debug\", value=False)\n    st.sidebar.toggle(\"Usar historial de chat\", key=\"use_chat_history\", value=True)\n\n    with st.sidebar.expander(\"Opciones avanzadas\"):\n        st.selectbox(\"Seleccionar modelo:\", MODELS, key=\"model_name\")\n        st.number_input(\n            \"Seleccionar número de fragmentos de contexto\",\n            value=5,\n            key=\"num_retrieved_chunks\",\n            min_value=1,\n            max_value=10,\n        )\n        st.number_input(\n            \"Seleccionar número de mensajes a usar en el historial de chat\",\n            value=5,\n            key=\"num_chat_messages\",\n            min_value=1,\n            max_value=10,\n        )\n\n    st.sidebar.expander(\"Estado de Sesión\").write(st.session_state)\n\ndef query_cortex_search_service(query):\n    \"\"\"\n    Consulta el servicio de búsqueda cortex seleccionado con la consulta dada y recupera documentos de contexto.\n    Muestra los documentos de contexto recuperados en la barra lateral si el modo debug está habilitado. Devuelve\n    los documentos de contexto como una cadena.\n\n    Args:\n        query (str): La consulta para buscar en el servicio de búsqueda cortex.\n\n    Returns:\n        str: La cadena concatenada de documentos de contexto.\n    \"\"\"\n    db, schema = session.get_current_database(), session.get_current_schema()\n\n    cortex_search_service = (\n        root.databases[db]\n        .schemas[schema]\n        .cortex_search_services[st.session_state.selected_cortex_search_service]\n    )\n\n    context_documents = cortex_search_service.search(\n        query, columns=[], limit=st.session_state.num_retrieved_chunks\n    )\n    results = context_documents.results\n\n    service_metadata = st.session_state.service_metadata\n    search_col = [s[\"search_column\"] for s in service_metadata\n                    if s[\"name\"] == st.session_state.selected_cortex_search_service][0]\n\n    context_str = \"\"\n    for i, r in enumerate(results):\n        context_str += f\"Documento de contexto {i+1}: {r[search_col]} \\n\" + \"\\n\"\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\"Documentos de contexto\", context_str, height=500)\n\n    return context_str\n\ndef get_chat_history():\n    \"\"\"\n    Recupera el historial de chat del estado de sesión limitado al número de mensajes especificado\n    por el usuario en las opciones de la barra lateral.\n\n    Returns:\n        list: La lista de mensajes de chat del estado de sesión.\n    \"\"\"\n    start_index = max(\n        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n    )\n    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n\ndef complete(model, prompt):\n    \"\"\"\n    Genera una respuesta para el prompt dado usando el modelo especificado.\n\n    Args:\n        model (str): El nombre del modelo a usar para la respuesta.\n        prompt (str): El prompt para generar una respuesta.\n\n    Returns:\n        str: La respuesta generada.\n    \"\"\"\n    return session.sql(\"SELECT ai_complete(?,?)\", (model, prompt)).collect()[0][0]\n\ndef make_chat_history_summary(chat_history, question):\n    \"\"\"\n    Genera un resumen del historial de chat combinado con la pregunta actual para extender el contexto\n    de la consulta. Usa el modelo de lenguaje para generar este resumen.\n\n    Args:\n        chat_history (str): El historial de chat a incluir en el resumen.\n        question (str): La pregunta actual del usuario para extender con el historial de chat.\n\n    Returns:\n        str: El resumen generado del historial de chat y la pregunta.\n    \"\"\"\n    prompt = f\"\"\"\n        [INST]\n        Basándote en el historial de chat a continuación y la pregunta, genera una consulta que extienda la pregunta\n        con el historial de chat proporcionado. La consulta debe estar en lenguaje natural.\n        Responde solo con la consulta. No agregues ninguna explicación.\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        [/INST]\n    \"\"\"\n\n    summary = complete(st.session_state.model_name, prompt)\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\n            \"Resumen del historial de chat\", summary.replace(\"$\", \"\\$\"), height=150\n        )\n\n    return summary\n\ndef create_prompt(user_question):\n    \"\"\"\n    Crea un prompt para el modelo de lenguaje combinando la pregunta del usuario con el contexto recuperado\n    del servicio de búsqueda cortex y el historial de chat (si está habilitado). Formatea el prompt de acuerdo\n    al formato de entrada esperado del modelo.\n\n    Args:\n        user_question (str): La pregunta del usuario para generar un prompt.\n\n    Returns:\n        str: El prompt generado para el modelo de lenguaje.\n    \"\"\"\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n        if chat_history != []:\n            question_summary = make_chat_history_summary(chat_history, user_question)\n            prompt_context = query_cortex_search_service(question_summary)\n        else:\n            prompt_context = query_cortex_search_service(user_question)\n    else:\n        prompt_context = query_cortex_search_service(user_question)\n        chat_history = \"\"\n\n    prompt = f\"\"\"\n            [INST]\n            Eres un asistente de chat de IA útil con capacidades RAG. Cuando un usuario te haga una pregunta,\n            también se te dará contexto proporcionado entre las etiquetas <context> y </context>. Usa ese contexto\n            con el historial de chat del usuario proporcionado entre las etiquetas <chat_history> y </chat_history>\n            para proporcionar un resumen que aborde la pregunta del usuario. Asegúrate de que la respuesta sea coherente, concisa\n            y directamente relevante a la pregunta del usuario.\n\n            Si el usuario hace una pregunta genérica que no puede ser respondida con el contexto dado o el chat_history,\n            simplemente di \"No conozco la respuesta a esa pregunta.\"\n\n            No digas cosas como \"según el contexto proporcionado\".\n\n            <chat_history>\n            {chat_history}\n            </chat_history>\n            <context>\n            {prompt_context}\n            </context>\n            <question>\n            {user_question}\n            </question>\n            [/INST]\n            Respuesta:\n        \"\"\"\n    return prompt\n\ndef main():\n    st.title(f\":speech_balloon: Chatbot con Snowflake Cortex\")\n\n    init_service_metadata()\n    init_config_options()\n    init_messages()\n\n    icons = {\"assistant\": \"❄️\", \"user\": \"👤\"}\n\n    # Mostrar mensajes de chat del historial al reejecutar la aplicación\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n            st.markdown(message[\"content\"])\n\n    disable_chat = (\n        \"service_metadata\" not in st.session_state\n        or len(st.session_state.service_metadata) == 0\n    )\n    if question := st.chat_input(\"Haz una pregunta...\", disabled=disable_chat):\n        # Agregar mensaje del usuario al historial de chat\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Mostrar mensaje del usuario en el contenedor de mensajes de chat\n        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n            st.markdown(question.replace(\"$\", \"\\$\"))\n\n        # Mostrar respuesta del asistente en el contenedor de mensajes de chat\n        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n            message_placeholder = st.empty()\n            question = question.replace(\"'\", \"\")\n            with st.spinner(\"Pensando...\"):\n                generated_response = complete(\n                    st.session_state.model_name, create_prompt(question)\n                )\n                message_placeholder.markdown(generated_response)\n\n        st.session_state.messages.append(\n            {\"role\": \"assistant\", \"content\": generated_response}\n        )\n\nif __name__ == \"__main__\":\n    session = get_active_session()\n    root = Root(session)\n    main()",
   "execution_count": null
  }
 ]
}