{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6qb2na7witd3tupol3dx",
   "authorId": "5639445461508",
   "authorName": "JOSE",
   "authorEmail": "jmoyacurbelo@outlook.com",
   "sessionId": "27f64567-c63a-4037-b05c-71b0d0061356",
   "lastEditTime": 1749596904858
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9471e55f-2d1f-4e58-9fda-548591d0e732",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "En esta lección veremos cómo crear un chatbot a partir de un conjunto de datos de documentos PDF con Cortex Search. Mostraremos un ejemplo de extracción de texto de los PDF mediante una UDF básica de Python y, posteriormente, la ingesta de los datos extraídos en un servicio de Cortex Search.\n\n### Cargar los datos en Snowflake\n\nPrimero, vamos a crear un stage para almacenar los archivos que contienen los datos. Este stage contendrá los archivos PDF que se encuentran adjuntos como recurso a esta lección."
  },
  {
   "cell_type": "code",
   "id": "c83a66a3-86ab-4dee-8057-4bb6e1797f02",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STAGE curso_ia.seccion_4.datos_aws\n    DIRECTORY = (ENABLE = TRUE)\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25865a75-5294-4db6-b921-47a2eff89480",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Cargar los PDFs al stage\n\nA continuación deberán cargar los PDFs adjuntos como recurso a esta lección al stage que acabamos de crear.\n\n### Parsear los archivos PDF\n\nEn este paso, extraeremos el texto sin procesar de los PDF y lo dividiremos en fragmentos para su incorporación al servicio de búsqueda.\n\nPrimero, usaremos la función `PARSE_DOCUMENT` de Cortex para extraer el texto de los PDF en una nueva tabla: `AWS_RAW_TEXT`."
  },
  {
   "cell_type": "code",
   "id": "39007b7f-9736-4e39-acdc-40202d1c0f8e",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE curso_ia.seccion_4.aws_raw_text AS\nSELECT\n    RELATIVE_PATH,\n    TO_VARCHAR (\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n            '@curso_ia.seccion_4.datos_aws',\n            RELATIVE_PATH,\n            {'mode': 'LAYOUT'} ):content\n        ) AS EXTRACTED_LAYOUT\nFROM\n    DIRECTORY('@curso_ia.seccion_4.datos_aws')\nWHERE\n    RELATIVE_PATH LIKE '%.pdf';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4a65b2f-68a3-40b7-b1bc-e3ab0a7fc0ae",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "SELECT * FROM curso_ia.seccion_4.aws_raw_text;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bedd332e-38e3-4806-985e-69d52b399817",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "A continuación, utilizaremos `SPLIT_TEXT_RECURSIVE_CHARACTER` para dividir los documentos en fragmentos de un tamaño máximo de 2000 caracteres cada uno, insertando los fragmentos en una nueva tabla `AWS_CHUNKS`."
  },
  {
   "cell_type": "code",
   "id": "6e07dee5-489e-4046-aa0d-b913cf6ecf26",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE curso_ia.seccion_4.aws_chunks AS\nSELECT\n    relative_path,\n    BUILD_SCOPED_FILE_URL(@datos_aws, relative_path, TRUE) AS file_url,\n    CONCAT(relative_path, ': ', c.value::TEXT) AS chunk,\n    'Español' AS language\nFROM\n    curso_ia.seccion_4.aws_raw_text,\n    LATERAL FLATTEN(SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n        EXTRACTED_LAYOUT,\n        'markdown',\n        2000, -- chunks de 2000 caracteres\n        300 -- 300 superposición de caracteres\n    )) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5443376-29ab-4a7f-a52a-bbdf25d77268",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "SELECT * FROM curso_ia.seccion_4.aws_chunks;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54d272d1-bb1b-4483-899a-82cfc37f0aed",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "### Crear el servicio de Cortex Search\n\nAhora crearemos un servicio de búsqueda de Cortex en nuestra nueva tabla ejecutando el siguiente comando SQL."
  },
  {
   "cell_type": "code",
   "id": "26c9e596-27f3-4d00-9be7-a2c249030705",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE curso_ia.seccion_4.aws_search_service\n    ON chunk\n    ATTRIBUTES language\n    WAREHOUSE = search_service_wh\n    TARGET_LAG = '6 hour',\n    EMBEDDING_MODEL = 'voyage-multilingual-2'\n    AS (\n    SELECT\n        chunk,\n        relative_path,\n        file_url,\n        language\n    FROM curso_ia.seccion_4.aws_chunks\n    );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea5f25cc-20a7-486f-a8af-a43d05999294",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "-- Mostramos los servicios de Cortex Search que tengamos creados\nSHOW CORTEX SEARCH SERVICES;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "400544aa-d933-45da-89be-721a527cfa04",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Crear una aplicación de Stremlit\n\nPodemos consultar el servicio creado con el SDK de Python (usando el paquete de Python Snowflake). En esta lección mostraremos el uso del SDK de Python en una aplicación Streamlit en Snowflake.\n\nA continuación deberá ir a la sección de Streamlit y seguir los siguientes pasos para crear la aplicación:\n\n1. Click en + Streamlit app\n2. Proporcionarle el nombre `aws_app`\n3. Seleccionar la base de datos, esquema y warehouse:\n    \n    Base de datos: `curso_ia`\n    \n    Esquema: `seccion_4`\n    \n4. Click en Create\n5. Eliminar el código de ejemplo que trae la plantilla\n6. Copiar el código Python de la siguiente celda en la aplicación de Streamlit.\n7. Agregar las librerías necesarias. Para esta aplicación debemos asegurarnos de que tenemos agregada las siguientes librerías:\n    - `snowflake`\n    - `snowflake-snowpark-python`\n8. Ejecutar la aplicación"
  },
  {
   "cell_type": "code",
   "id": "2a6e63c2-d783-46de-a725-0cc7aa6ff4c2",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.core import Root # requiere snowflake>=0.8.0\nfrom snowflake.snowpark.context import get_active_session\n\nMODELS = [\n    \"claude-4-sonnet\",\n    \"openai-gpt-4.1\",\n    \"mistral-large\",\n    \"snowflake-arctic\",\n    \"llama3-70b\",\n    \"llama3-8b\",\n]\n\ndef init_messages():\n    \"\"\"\n    Inicializa el estado de sesión para los mensajes del chat. Si el estado de sesión indica que la\n    conversación debe ser borrada o si la clave \"messages\" no está en el estado de sesión,\n    la inicializa como una lista vacía.\n    \"\"\"\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n\ndef init_service_metadata():\n    \"\"\"\n    Inicializa el estado de sesión para los metadatos del servicio de búsqueda cortex. Consulta los\n    servicios de búsqueda cortex disponibles desde la sesión de Snowflake y almacena sus nombres y\n    columnas de búsqueda en el estado de sesión.\n    \"\"\"\n    if \"service_metadata\" not in st.session_state:\n        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n        service_metadata = []\n        if services:\n            for s in services:\n                svc_name = s[\"name\"]\n                svc_search_col = session.sql(\n                    f\"DESC CORTEX SEARCH SERVICE {svc_name};\"\n                ).collect()[0][\"search_column\"]\n                service_metadata.append(\n                    {\"name\": svc_name, \"search_column\": svc_search_col}\n                )\n\n        st.session_state.service_metadata = service_metadata\n\n\ndef init_config_options():\n    \"\"\"\n    Inicializa las opciones de configuración en la barra lateral de Streamlit. Permite al usuario seleccionar\n    un servicio de búsqueda cortex, borrar la conversación, alternar el modo debug y alternar el uso del\n    historial de chat. También proporciona opciones avanzadas para seleccionar un modelo, el número de\n    fragmentos de contexto y el número de mensajes de chat a usar en el historial de chat.\n    \"\"\"\n    st.sidebar.selectbox(\n        \"Seleccionar servicio de búsqueda cortex:\",\n        [s[\"name\"] for s in st.session_state.service_metadata],\n        key=\"selected_cortex_search_service\",\n    )\n\n    st.sidebar.button(\"Borrar conversación\", key=\"clear_conversation\")\n    st.sidebar.toggle(\"Debug\", key=\"debug\", value=False)\n    st.sidebar.toggle(\"Usar historial de chat\", key=\"use_chat_history\", value=True)\n\n    with st.sidebar.expander(\"Opciones avanzadas\"):\n        st.selectbox(\"Seleccionar modelo:\", MODELS, key=\"model_name\")\n        st.number_input(\n            \"Seleccionar número de fragmentos de contexto\",\n            value=5,\n            key=\"num_retrieved_chunks\",\n            min_value=1,\n            max_value=10,\n        )\n        st.number_input(\n            \"Seleccionar número de mensajes a usar en el historial de chat\",\n            value=5,\n            key=\"num_chat_messages\",\n            min_value=1,\n            max_value=10,\n        )\n\n    st.sidebar.expander(\"Estado de Sesión\").write(st.session_state)\n\n\ndef query_cortex_search_service(query, columns = [], filter={}):\n    \"\"\"\n    Consulta el servicio de búsqueda cortex seleccionado con la consulta dada y recupera documentos de contexto.\n    Muestra los documentos de contexto recuperados en la barra lateral si el modo debug está habilitado. Devuelve\n    los documentos de contexto como una cadena.\n\n    Args:\n        query (str): La consulta para buscar en el servicio de búsqueda cortex.\n\n    Returns:\n        str: La cadena concatenada de documentos de contexto.\n    \"\"\"\n    db, schema = session.get_current_database(), session.get_current_schema()\n\n    cortex_search_service = (\n        root.databases[db]\n        .schemas[schema]\n        .cortex_search_services[st.session_state.selected_cortex_search_service]\n    )\n\n    context_documents = cortex_search_service.search(\n        query, columns=columns, filter=filter, limit=st.session_state.num_retrieved_chunks\n    )\n    results = context_documents.results\n\n    service_metadata = st.session_state.service_metadata\n    search_col = [s[\"search_column\"] for s in service_metadata\n                    if s[\"name\"] == st.session_state.selected_cortex_search_service][0].lower()\n\n    context_str = \"\"\n    for i, r in enumerate(results):\n        context_str += f\"Documento de contexto {i+1}: {r[search_col]} \\n\" + \"\\n\"\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\"Documentos de contexto\", context_str, height=500)\n\n    return context_str, results\n\n\ndef get_chat_history():\n    \"\"\"\n    Recupera el historial de chat del estado de sesión limitado al número de mensajes especificado\n    por el usuario en las opciones de la barra lateral.\n\n    Returns:\n        list: La lista de mensajes de chat del estado de sesión.\n    \"\"\"\n    start_index = max(\n        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n    )\n    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n\n\ndef complete(model, prompt):\n    \"\"\"\n    Genera una respuesta para el prompt dado usando el modelo especificado.\n\n    Args:\n        model (str): El nombre del modelo a usar para la respuesta.\n        prompt (str): El prompt para generar una respuesta.\n\n    Returns:\n        str: La respuesta generada.\n    \"\"\"\n    return session.sql(\"SELECT ai_complete(?,?)\", (model, prompt)).collect()[0][0]\n\ndef make_chat_history_summary(chat_history, question):\n    \"\"\"\n    Genera un resumen del historial de chat combinado con la pregunta actual para extender el contexto\n    de la consulta. Usa el modelo de lenguaje para generar este resumen.\n\n    Args:\n        chat_history (str): El historial de chat a incluir en el resumen.\n        question (str): La pregunta actual del usuario para extender con el historial de chat.\n\n    Returns:\n        str: El resumen generado del historial de chat y la pregunta.\n    \"\"\"\n    prompt = f\"\"\"\n        [INST]\n        Basándote en el historial de chat a continuación y la pregunta, genera una consulta que extienda la pregunta\n        con el historial de chat proporcionado. La consulta debe estar en lenguaje natural.\n        Responde solo con la consulta. No agregues ninguna explicación.\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        [/INST]\n    \"\"\"\n\n    summary = complete(st.session_state.model_name, prompt)\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\n            \"Resumen del historial de chat\", summary.replace(\"$\", \"\\$\"), height=150\n        )\n\n    return summary\n\n\ndef create_prompt(user_question):\n    \"\"\"\n    Crea un prompt para el modelo de lenguaje combinando la pregunta del usuario con el contexto recuperado\n    del servicio de búsqueda cortex y el historial de chat (si está habilitado). Formatea el prompt de acuerdo\n    al formato de entrada esperado del modelo.\n\n    Args:\n        user_question (str): La pregunta del usuario para generar un prompt.\n\n    Returns:\n        str: El prompt generado para el modelo de lenguaje.\n    \"\"\"\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n        if chat_history != []:\n            question_summary = make_chat_history_summary(chat_history, user_question)\n            prompt_context, results = query_cortex_search_service(\n                question_summary,\n                columns=[\"chunk\", \"file_url\", \"relative_path\"],\n                filter={\"@and\": [{\"@eq\": {\"language\": \"Español\"}}]},\n            )\n        else:\n            prompt_context, results = query_cortex_search_service(\n                user_question,\n                columns=[\"chunk\", \"file_url\", \"relative_path\"],\n                filter={\"@and\": [{\"@eq\": {\"language\": \"Español\"}}]},\n            )\n    else:\n        prompt_context, results = query_cortex_search_service(\n            user_question,\n            columns=[\"chunk\", \"file_url\", \"relative_path\"],\n            filter={\"@and\": [{\"@eq\": {\"language\": \"Español\"}}]},\n        )\n        chat_history = \"\"\n\n    prompt = f\"\"\"\n        [INST]\n        Eres un asistente de chat de IA útil con capacidades RAG. Cuando un usuario te haga una pregunta,\n        también se te dará contexto proporcionado entre las etiquetas <context> y </context>. Usa ese contexto\n        con el historial de chat del usuario proporcionado entre las etiquetas <chat_history> y </chat_history>\n        para proporcionar un resumen que aborde la pregunta del usuario. \n\n        IMPORTANTE: Formatea tu respuesta usando Markdown apropiado:\n        - Usa líneas vacías (doble salto de línea) entre párrafos\n        - Usa **texto** para negritas\n        - Usa listas con - para elementos\n        - Usa ### para subtítulos cuando sea apropiado\n        \n        Asegúrate de que la respuesta sea coherente, concisa y directamente relevante a la pregunta del usuario.\n\n        Si el usuario hace una pregunta genérica que no puede ser respondida con el contexto dado o el chat_history,\n        simplemente di \"No conozco la respuesta a esa pregunta.\"\n\n        No digas cosas como \"según el contexto proporcionado\".\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <context>\n        {prompt_context}\n        </context>\n        <question>\n        {user_question}\n        </question>\n        [/INST]\n        Respuesta:\n        \"\"\"\n    return prompt, results\n\n\ndef main():\n    st.title(f\":speech_balloon: Chatbot con Snowflake Cortex\")\n\n    init_service_metadata()\n    init_config_options()\n    init_messages()\n\n    icons = {\"assistant\": \"❄️\", \"user\": \"👤\"}\n\n    # Mostrar mensajes de chat del historial al reejecutar la aplicación\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n            st.markdown(message[\"content\"])\n\n    disable_chat = (\n        \"service_metadata\" not in st.session_state\n        or len(st.session_state.service_metadata) == 0\n    )\n    if question := st.chat_input(\"Haz una pregunta...\", disabled=disable_chat):\n        # Agregar mensaje del usuario al historial de chat\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Mostrar mensaje del usuario en el contenedor de mensajes de chat\n        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n            st.markdown(question.replace(\"$\", \"\\$\"))\n\n        # Mostrar respuesta del asistente en el contenedor de mensajes de chat\n        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n            message_placeholder = st.empty()\n            question = question.replace(\"'\", \"\")\n            prompt, results = create_prompt(question)\n            with st.spinner(\"Pensando...\"):\n                generated_response = complete(\n                    st.session_state.model_name, prompt\n                )\n                # construir tabla de referencias para citación\n                markdown_table = \"###### Referencias \\n\\n| Título PDF |\\n|-------|\\n\"\n                for ref in results:\n                    markdown_table += f\"| {ref['relative_path']} |\\n\"\n                generated_response_clean = generated_response.replace(\"\\\\n\", \"\\n\").replace(\"\\\\\", \"\")\n                message_placeholder.markdown(generated_response_clean + \"\\n\\n\" + markdown_table)\n                \n\n        st.session_state.messages.append(\n            {\"role\": \"assistant\", \"content\": generated_response}\n        )\n\n\nif __name__ == \"__main__\":\n    session = get_active_session()\n    root = Root(session)\n    main()",
   "execution_count": null
  }
 ]
}