{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6qb2na7witd3tupol3dx",
   "authorId": "5639445461508",
   "authorName": "JOSE",
   "authorEmail": "jmoyacurbelo@outlook.com",
   "sessionId": "27f64567-c63a-4037-b05c-71b0d0061356",
   "lastEditTime": 1749596904858
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9471e55f-2d1f-4e58-9fda-548591d0e732",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "En esta lecci칩n veremos c칩mo crear un chatbot a partir de un conjunto de datos de documentos PDF con Cortex Search. Mostraremos un ejemplo de extracci칩n de texto de los PDF mediante una UDF b치sica de Python y, posteriormente, la ingesta de los datos extra칤dos en un servicio de Cortex Search.\n\n### Cargar los datos en Snowflake\n\nPrimero, vamos a crear un stage para almacenar los archivos que contienen los datos. Este stage contendr치 los archivos PDF que se encuentran adjuntos como recurso a esta lecci칩n."
  },
  {
   "cell_type": "code",
   "id": "c83a66a3-86ab-4dee-8057-4bb6e1797f02",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STAGE curso_ia.seccion_4.datos_aws\n    DIRECTORY = (ENABLE = TRUE)\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25865a75-5294-4db6-b921-47a2eff89480",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Cargar los PDFs al stage\n\nA continuaci칩n deber치n cargar los PDFs adjuntos como recurso a esta lecci칩n al stage que acabamos de crear.\n\n### Parsear los archivos PDF\n\nEn este paso, extraeremos el texto sin procesar de los PDF y lo dividiremos en fragmentos para su incorporaci칩n al servicio de b칰squeda.\n\nPrimero, usaremos la funci칩n `PARSE_DOCUMENT` de Cortex para extraer el texto de los PDF en una nueva tabla: `AWS_RAW_TEXT`."
  },
  {
   "cell_type": "code",
   "id": "39007b7f-9736-4e39-acdc-40202d1c0f8e",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE curso_ia.seccion_4.aws_raw_text AS\nSELECT\n    RELATIVE_PATH,\n    TO_VARCHAR (\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n            '@curso_ia.seccion_4.datos_aws',\n            RELATIVE_PATH,\n            {'mode': 'LAYOUT'} ):content\n        ) AS EXTRACTED_LAYOUT\nFROM\n    DIRECTORY('@curso_ia.seccion_4.datos_aws')\nWHERE\n    RELATIVE_PATH LIKE '%.pdf';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4a65b2f-68a3-40b7-b1bc-e3ab0a7fc0ae",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "SELECT * FROM curso_ia.seccion_4.aws_raw_text;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bedd332e-38e3-4806-985e-69d52b399817",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "A continuaci칩n, utilizaremos `SPLIT_TEXT_RECURSIVE_CHARACTER` para dividir los documentos en fragmentos de un tama침o m치ximo de 2000 caracteres cada uno, insertando los fragmentos en una nueva tabla `AWS_CHUNKS`."
  },
  {
   "cell_type": "code",
   "id": "6e07dee5-489e-4046-aa0d-b913cf6ecf26",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE curso_ia.seccion_4.aws_chunks AS\nSELECT\n    relative_path,\n    BUILD_SCOPED_FILE_URL(@datos_aws, relative_path, TRUE) AS file_url,\n    CONCAT(relative_path, ': ', c.value::TEXT) AS chunk,\n    'Espa침ol' AS language\nFROM\n    curso_ia.seccion_4.aws_raw_text,\n    LATERAL FLATTEN(SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n        EXTRACTED_LAYOUT,\n        'markdown',\n        2000, -- chunks de 2000 caracteres\n        300 -- 300 superposici칩n de caracteres\n    )) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5443376-29ab-4a7f-a52a-bbdf25d77268",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "SELECT * FROM curso_ia.seccion_4.aws_chunks;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54d272d1-bb1b-4483-899a-82cfc37f0aed",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "### Crear el servicio de Cortex Search\n\nAhora crearemos un servicio de b칰squeda de Cortex en nuestra nueva tabla ejecutando el siguiente comando SQL."
  },
  {
   "cell_type": "code",
   "id": "26c9e596-27f3-4d00-9be7-a2c249030705",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE curso_ia.seccion_4.aws_search_service\n    ON chunk\n    ATTRIBUTES language\n    WAREHOUSE = search_service_wh\n    TARGET_LAG = '6 hour',\n    EMBEDDING_MODEL = 'voyage-multilingual-2'\n    AS (\n    SELECT\n        chunk,\n        relative_path,\n        file_url,\n        language\n    FROM curso_ia.seccion_4.aws_chunks\n    );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea5f25cc-20a7-486f-a8af-a43d05999294",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "-- Mostramos los servicios de Cortex Search que tengamos creados\nSHOW CORTEX SEARCH SERVICES;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "400544aa-d933-45da-89be-721a527cfa04",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Crear una aplicaci칩n de Stremlit\n\nPodemos consultar el servicio creado con el SDK de Python (usando el paquete de Python Snowflake). En esta lecci칩n mostraremos el uso del SDK de Python en una aplicaci칩n Streamlit en Snowflake.\n\nA continuaci칩n deber치 ir a la secci칩n de Streamlit y seguir los siguientes pasos para crear la aplicaci칩n:\n\n1. Click en + Streamlit app\n2. Proporcionarle el nombre `aws_app`\n3. Seleccionar la base de datos, esquema y warehouse:\n    \n    Base de datos: `curso_ia`\n    \n    Esquema: `seccion_4`\n    \n4. Click en Create\n5. Eliminar el c칩digo de ejemplo que trae la plantilla\n6. Copiar el c칩digo Python de la siguiente celda en la aplicaci칩n de Streamlit.\n7. Agregar las librer칤as necesarias. Para esta aplicaci칩n debemos asegurarnos de que tenemos agregada las siguientes librer칤as:\n    - `snowflake`\n    - `snowflake-snowpark-python`\n8. Ejecutar la aplicaci칩n"
  },
  {
   "cell_type": "code",
   "id": "2a6e63c2-d783-46de-a725-0cc7aa6ff4c2",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.core import Root # requiere snowflake>=0.8.0\nfrom snowflake.snowpark.context import get_active_session\n\nMODELS = [\n    \"claude-4-sonnet\",\n    \"openai-gpt-4.1\",\n    \"mistral-large\",\n    \"snowflake-arctic\",\n    \"llama3-70b\",\n    \"llama3-8b\",\n]\n\ndef init_messages():\n    \"\"\"\n    Inicializa el estado de sesi칩n para los mensajes del chat. Si el estado de sesi칩n indica que la\n    conversaci칩n debe ser borrada o si la clave \"messages\" no est치 en el estado de sesi칩n,\n    la inicializa como una lista vac칤a.\n    \"\"\"\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n\ndef init_service_metadata():\n    \"\"\"\n    Inicializa el estado de sesi칩n para los metadatos del servicio de b칰squeda cortex. Consulta los\n    servicios de b칰squeda cortex disponibles desde la sesi칩n de Snowflake y almacena sus nombres y\n    columnas de b칰squeda en el estado de sesi칩n.\n    \"\"\"\n    if \"service_metadata\" not in st.session_state:\n        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n        service_metadata = []\n        if services:\n            for s in services:\n                svc_name = s[\"name\"]\n                svc_search_col = session.sql(\n                    f\"DESC CORTEX SEARCH SERVICE {svc_name};\"\n                ).collect()[0][\"search_column\"]\n                service_metadata.append(\n                    {\"name\": svc_name, \"search_column\": svc_search_col}\n                )\n\n        st.session_state.service_metadata = service_metadata\n\n\ndef init_config_options():\n    \"\"\"\n    Inicializa las opciones de configuraci칩n en la barra lateral de Streamlit. Permite al usuario seleccionar\n    un servicio de b칰squeda cortex, borrar la conversaci칩n, alternar el modo debug y alternar el uso del\n    historial de chat. Tambi칠n proporciona opciones avanzadas para seleccionar un modelo, el n칰mero de\n    fragmentos de contexto y el n칰mero de mensajes de chat a usar en el historial de chat.\n    \"\"\"\n    st.sidebar.selectbox(\n        \"Seleccionar servicio de b칰squeda cortex:\",\n        [s[\"name\"] for s in st.session_state.service_metadata],\n        key=\"selected_cortex_search_service\",\n    )\n\n    st.sidebar.button(\"Borrar conversaci칩n\", key=\"clear_conversation\")\n    st.sidebar.toggle(\"Debug\", key=\"debug\", value=False)\n    st.sidebar.toggle(\"Usar historial de chat\", key=\"use_chat_history\", value=True)\n\n    with st.sidebar.expander(\"Opciones avanzadas\"):\n        st.selectbox(\"Seleccionar modelo:\", MODELS, key=\"model_name\")\n        st.number_input(\n            \"Seleccionar n칰mero de fragmentos de contexto\",\n            value=5,\n            key=\"num_retrieved_chunks\",\n            min_value=1,\n            max_value=10,\n        )\n        st.number_input(\n            \"Seleccionar n칰mero de mensajes a usar en el historial de chat\",\n            value=5,\n            key=\"num_chat_messages\",\n            min_value=1,\n            max_value=10,\n        )\n\n    st.sidebar.expander(\"Estado de Sesi칩n\").write(st.session_state)\n\n\ndef query_cortex_search_service(query, columns = [], filter={}):\n    \"\"\"\n    Consulta el servicio de b칰squeda cortex seleccionado con la consulta dada y recupera documentos de contexto.\n    Muestra los documentos de contexto recuperados en la barra lateral si el modo debug est치 habilitado. Devuelve\n    los documentos de contexto como una cadena.\n\n    Args:\n        query (str): La consulta para buscar en el servicio de b칰squeda cortex.\n\n    Returns:\n        str: La cadena concatenada de documentos de contexto.\n    \"\"\"\n    db, schema = session.get_current_database(), session.get_current_schema()\n\n    cortex_search_service = (\n        root.databases[db]\n        .schemas[schema]\n        .cortex_search_services[st.session_state.selected_cortex_search_service]\n    )\n\n    context_documents = cortex_search_service.search(\n        query, columns=columns, filter=filter, limit=st.session_state.num_retrieved_chunks\n    )\n    results = context_documents.results\n\n    service_metadata = st.session_state.service_metadata\n    search_col = [s[\"search_column\"] for s in service_metadata\n                    if s[\"name\"] == st.session_state.selected_cortex_search_service][0].lower()\n\n    context_str = \"\"\n    for i, r in enumerate(results):\n        context_str += f\"Documento de contexto {i+1}: {r[search_col]} \\n\" + \"\\n\"\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\"Documentos de contexto\", context_str, height=500)\n\n    return context_str, results\n\n\ndef get_chat_history():\n    \"\"\"\n    Recupera el historial de chat del estado de sesi칩n limitado al n칰mero de mensajes especificado\n    por el usuario en las opciones de la barra lateral.\n\n    Returns:\n        list: La lista de mensajes de chat del estado de sesi칩n.\n    \"\"\"\n    start_index = max(\n        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n    )\n    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n\n\ndef complete(model, prompt):\n    \"\"\"\n    Genera una respuesta para el prompt dado usando el modelo especificado.\n\n    Args:\n        model (str): El nombre del modelo a usar para la respuesta.\n        prompt (str): El prompt para generar una respuesta.\n\n    Returns:\n        str: La respuesta generada.\n    \"\"\"\n    return session.sql(\"SELECT ai_complete(?,?)\", (model, prompt)).collect()[0][0]\n\ndef make_chat_history_summary(chat_history, question):\n    \"\"\"\n    Genera un resumen del historial de chat combinado con la pregunta actual para extender el contexto\n    de la consulta. Usa el modelo de lenguaje para generar este resumen.\n\n    Args:\n        chat_history (str): El historial de chat a incluir en el resumen.\n        question (str): La pregunta actual del usuario para extender con el historial de chat.\n\n    Returns:\n        str: El resumen generado del historial de chat y la pregunta.\n    \"\"\"\n    prompt = f\"\"\"\n        [INST]\n        Bas치ndote en el historial de chat a continuaci칩n y la pregunta, genera una consulta que extienda la pregunta\n        con el historial de chat proporcionado. La consulta debe estar en lenguaje natural.\n        Responde solo con la consulta. No agregues ninguna explicaci칩n.\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        [/INST]\n    \"\"\"\n\n    summary = complete(st.session_state.model_name, prompt)\n\n    if st.session_state.debug:\n        st.sidebar.text_area(\n            \"Resumen del historial de chat\", summary.replace(\"$\", \"\\$\"), height=150\n        )\n\n    return summary\n\n\ndef create_prompt(user_question):\n    \"\"\"\n    Crea un prompt para el modelo de lenguaje combinando la pregunta del usuario con el contexto recuperado\n    del servicio de b칰squeda cortex y el historial de chat (si est치 habilitado). Formatea el prompt de acuerdo\n    al formato de entrada esperado del modelo.\n\n    Args:\n        user_question (str): La pregunta del usuario para generar un prompt.\n\n    Returns:\n        str: El prompt generado para el modelo de lenguaje.\n    \"\"\"\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n        if chat_history != []:\n            question_summary = make_chat_history_summary(chat_history, user_question)\n            prompt_context, results = query_cortex_search_service(\n                question_summary,\n                columns=[\"chunk\", \"file_url\", \"relative_path\"],\n                filter={\"@and\": [{\"@eq\": {\"language\": \"Espa침ol\"}}]},\n            )\n        else:\n            prompt_context, results = query_cortex_search_service(\n                user_question,\n                columns=[\"chunk\", \"file_url\", \"relative_path\"],\n                filter={\"@and\": [{\"@eq\": {\"language\": \"Espa침ol\"}}]},\n            )\n    else:\n        prompt_context, results = query_cortex_search_service(\n            user_question,\n            columns=[\"chunk\", \"file_url\", \"relative_path\"],\n            filter={\"@and\": [{\"@eq\": {\"language\": \"Espa침ol\"}}]},\n        )\n        chat_history = \"\"\n\n    prompt = f\"\"\"\n        [INST]\n        Eres un asistente de chat de IA 칰til con capacidades RAG. Cuando un usuario te haga una pregunta,\n        tambi칠n se te dar치 contexto proporcionado entre las etiquetas <context> y </context>. Usa ese contexto\n        con el historial de chat del usuario proporcionado entre las etiquetas <chat_history> y </chat_history>\n        para proporcionar un resumen que aborde la pregunta del usuario. \n\n        IMPORTANTE: Formatea tu respuesta usando Markdown apropiado:\n        - Usa l칤neas vac칤as (doble salto de l칤nea) entre p치rrafos\n        - Usa **texto** para negritas\n        - Usa listas con - para elementos\n        - Usa ### para subt칤tulos cuando sea apropiado\n        \n        Aseg칰rate de que la respuesta sea coherente, concisa y directamente relevante a la pregunta del usuario.\n\n        Si el usuario hace una pregunta gen칠rica que no puede ser respondida con el contexto dado o el chat_history,\n        simplemente di \"No conozco la respuesta a esa pregunta.\"\n\n        No digas cosas como \"seg칰n el contexto proporcionado\".\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <context>\n        {prompt_context}\n        </context>\n        <question>\n        {user_question}\n        </question>\n        [/INST]\n        Respuesta:\n        \"\"\"\n    return prompt, results\n\n\ndef main():\n    st.title(f\":speech_balloon: Chatbot con Snowflake Cortex\")\n\n    init_service_metadata()\n    init_config_options()\n    init_messages()\n\n    icons = {\"assistant\": \"仇勇끂", \"user\": \"游녻\"}\n\n    # Mostrar mensajes de chat del historial al reejecutar la aplicaci칩n\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n            st.markdown(message[\"content\"])\n\n    disable_chat = (\n        \"service_metadata\" not in st.session_state\n        or len(st.session_state.service_metadata) == 0\n    )\n    if question := st.chat_input(\"Haz una pregunta...\", disabled=disable_chat):\n        # Agregar mensaje del usuario al historial de chat\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Mostrar mensaje del usuario en el contenedor de mensajes de chat\n        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n            st.markdown(question.replace(\"$\", \"\\$\"))\n\n        # Mostrar respuesta del asistente en el contenedor de mensajes de chat\n        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n            message_placeholder = st.empty()\n            question = question.replace(\"'\", \"\")\n            prompt, results = create_prompt(question)\n            with st.spinner(\"Pensando...\"):\n                generated_response = complete(\n                    st.session_state.model_name, prompt\n                )\n                # construir tabla de referencias para citaci칩n\n                markdown_table = \"###### Referencias \\n\\n| T칤tulo PDF |\\n|-------|\\n\"\n                for ref in results:\n                    markdown_table += f\"| {ref['relative_path']} |\\n\"\n                generated_response_clean = generated_response.replace(\"\\\\n\", \"\\n\").replace(\"\\\\\", \"\")\n                message_placeholder.markdown(generated_response_clean + \"\\n\\n\" + markdown_table)\n                \n\n        st.session_state.messages.append(\n            {\"role\": \"assistant\", \"content\": generated_response}\n        )\n\n\nif __name__ == \"__main__\":\n    session = get_active_session()\n    root = Root(session)\n    main()",
   "execution_count": null
  }
 ]
}